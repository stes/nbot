'''
Created on 23.07.2012

This file contains all necessary routines and classes to process a
given text and rate the relevance of the text for the user 

@author: stes
'''

from htmlparser import *
from numpy import *
from numpy.linalg import *
from tools import printlist
from ai.linearreg import LinearRegression
from nbot.document import preprocess, load_document

def gen_feature_vector(mask, text):
    '''
    Generates a feature vector by applying the given mask to the specified
    document.
    The feature vector is generated by mapping the words in the mask list to
    the corresponding number of occurrences of these words in the document.
    Therefore, the length of the mask specifies the length of the output list.
    See also: 'Bag-of-words model'
    
    @param mask: a list of words that should be used as the mask
    @param text: the text for which the feature vector should be generated
    
    @return: a list of integers representing occurrences of words in the
    document
    '''
    processed = preprocess(text)
    vlist = VocabList()
    vlist.expand_with(processed)
    fvector = []
    for word in mask:
        fvector.append(vlist.quantity_of(word) / float(vlist.get_total_word_count()))
    return fvector

def expand_features(X):
    '''
    Expands the given feature matrix by added new polynomial features.
    The single feature vectors have to be arranged in rows inside the matrix.
    
    This function multiplies each feature of a row vector with every other
    feature and appends the resulting values to the vector:
    
    (f1, f2, f3) => (f1, f2, f3, f1*f1, f1*f2, f1*f3, f2*f2, f2*f3, f3*f3)
    
    @param X: A matrix containing the feature vectors
    
    @return: The expanded matrix
    '''
    X_new = []
    for row in X:
        new_row = []
        new_row.extend(row)
        l = len(row)
        for i in range(l):
            for j in range(i, l):
                new_row.append(row[i]*row[j])
        X_new.append(new_row)
    return X_new

class RecommenderSystem():
    '''
    System to rate new pages and estimate the relevance for the user
    '''

    def __init__(self, mask, n):
        '''
        Constructor
        '''
        self.__wordmask = mask
        self.__lreg = LinearRegression(n)
        self.__training_set = []
        self.__ratings = []
    
    def rate(self, document):
        '''
        rates the specified document
        @return: a value between 0 and 1 that specifies how well this
        document suits to the user
        '''
        f = gen_feature_vector(self.__wordmask, document)
        h = self.__lreg.estimate(array([f]))
        return float(h[0][0])
    
    def set_rate(self, document, rating):
        '''
        Lets the user rate a particular document
        @param rating: The user rating, between 0 (no interest) and 1
        (great interest)
        '''
        f = gen_feature_vector(self.__wordmask, document)
        self.__training_set.append(f)
        self.__ratings.append([rating])
    
    def train(self, iterations, learnrate):
        '''
        Trains the classifier
        @param iterations: the number of iterations
        @param learnrate: the learning rate
        '''
        [X, Y] = self.__gen_matrix()
        print X
        print Y
        self.__lreg.train(iterations, learnrate, X, Y)
    
    def __gen_matrix(self):
        return [ array(self.__training_set), array(self.__ratings) ]                      
    
if __name__ == '__main__':
    # some tests
    
    from nbot.document import Document, Library, VocabList
    
    doc0 = load_document('res/sample/blubb.html')
    doc1 = load_document('res/sample/page.html')
    doc2 = load_document('res/sample/dislikepage.html')
    
    lib_like = Library()
    lib_like.load('res/like', False)
    
    lib_dislike = Library()
    lib_dislike.load('res/dislike', False)
    
    vlist_like = lib_like.gen_vocablist()
    vlist_dislike = lib_dislike.gen_vocablist()
    
    vlist_like.clean(10)
    vlist_dislike.clean(10)
    
    like_mask = vlist_like.gen_mask()
    dislike_mask = vlist_dislike.gen_mask()
    
    printlist(like_mask)
    print '-------------------------------------------'
    printlist(dislike_mask)
    
    mask = []
    mask.extend(like_mask)
    mask.extend(dislike_mask)
    
    rsys = RecommenderSystem(mask, len(mask))
    for key in lib_like.get_keys():
        doc = lib_like.get_document(key)
        rsys.set_rate(doc.content(), 1.)
    
    for key in lib_dislike.get_keys():
        doc = lib_dislike.get_document(key)
        rsys.set_rate(doc.content(), 0.)
    
    rsys.train(10000000, 0.1)

    for key in lib_like.get_keys()[:5]:
        doc = lib_like.get_document(key)
        print rsys.rate(doc.content())
    
    for key in lib_dislike.get_keys()[:5]:
        doc = lib_dislike.get_document(key)
        print rsys.rate(doc.content())
    
    print '---------------------------------------'
    print rsys.rate(doc0.content())
    print rsys.rate(doc1.content())
    print rsys.rate(doc2.content())
    
    '''
    texts = ['duck duck duck duck duck',
             'duck pidgin pidgin pidgin pidgin',
             'duck duck duck duck pidgin',
             'pidgin pidgin pidgin pidgin pidgin',
             'pidgin pidgin pidgin duck duck',
             'pidgin pidgin duck duck duck',
             'pidgin pidgin pig pig pig']
    ratings =[1.,
              0.1,
              0.8,
              0.,
              0.3,
              0.75,
              0.9]
    
    
    libr = Library()
    for i in range(len(texts)):
        libr.add_document(Document(texts[i]))
    
    vlist = libr.gen_vocablist()
    print vlist
    
    rsys = RecommenderSystem(vlist.gen_mask(), 9)
    for i in range(len(texts)):
        rsys.set_rate(texts[i], ratings[i])
    
    rsys.train(100000, 0.1)
    for i in range(len(texts)):
        print rsys.rate(texts[i])
    # obviously, the users likes ducks and pigs most ;)
    print rsys.rate('duck pig pig pig duck duck duck duck duck duck')
    
    '''
    '''
    content = fetch_content('codinghorror.com', '/blog')
    content = preprocess(content)
    tmp = ''
    for c in content:
        tmp+=c
        if len(tmp) > 80:
            print(tmp)
            tmp = ''
    vlist = VocabList()
    vlist.expand_with(content)
    vlist.clean(20)
    print vlist
    vlist.sort()
    print vlist.gen_mask()
    vec = gen_feature_vector(vlist.gen_mask(), "Because of something.")
    print vec
    '''